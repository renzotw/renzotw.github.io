---
layout: post
title: Multiple Regression Analysis in R
category: project
permalink: /projects/blog-post-67/
---

Multiple regression analysis to understand the factors affecting the pricing dynamics of the American car market. 

Finished top 10 in this Kaggle Data Sceience competition.

## Table of contents
1. [Problem Statement](#Problem-statement)
    - [Data Description](#Data-description)  
    - [Specifications](#Specifications)
2. [Packages](#Packages)
3. [EDA](#EDA)
    - [Categorical Variables](#Categorical-variables)  
    - [Numerical Variables](#Numerical-variables)
4. [Creating New Variables](#Creating-new-variables)
5. [Variable Selection](#Variable-selection)
6. [Testing and Fitting Model](#Testing-and-fitting-model)
7. [Reflections](#Reflections)

## Problem Statement <a name="Problem-statement"></a>
**Summary of problem statement**: A foreign automobile company aspires to enter the US market by setting up their manufacturing and production proccesses there to compete with their US and European counterparts. They want to understand the specific factors affecting the pricing of cars in the American market wiht a particular interest in independent variables that influence the prices, as well as how well these variables describe the price. The goal is to build a multiple linear regression model that optimizes for the highest R-Squared value using the least number of variables. 

### Data Description <a name="Data-description"></a>
The training data contains 1500 observations, and the testing data contains 500 obersvations. There are 23 independent variables and one target variable - the price. 

### Specifications <a name="Specifications"></a>
The model must be a MLR model checked for a linearity, homoscedasticity, no multi-colinearity between the independent variables, and normality, as well as a thorough residual analysis of the model.

## Packages <a name="Packages"></a>
The following packages were used for this project:
```{r}
library(readr)
library(corrplot)
library(ggplot2)
library(car)
library(leaps)
library(GGally)
library(caret)
```

## Exploratory Data Analysis <a name="EDA"></a>
### Categorical Variables <a name="Categorical-variables"></a>
The first step of the EDA process involved isolating the categorical variables to understand their relationship with price.

```{r}
# Identify categorical variables
unlist(lapply(cars.train, is.character))
```

![categorical.png](/projects/assets/images/categorical.png)

Then, I plotted the categorical distributions and price by each variable to understand their relationships. Here is a sample graph:

```{r}
ggplot(cars.train, aes(x=Type, y=PriceNew)) + geom_boxplot(aes(fill= Origin,shape = Origin))
```

![graph-1.png](/projects/assets/images/graph-1.png)

From the various visualizations, critical observations were made: 

1. Model and Make each have 93 unique categories that are mutually exclusive. This would result in too many betas and overfitting so we created a new categorical variable that synthesized this data. The four bins for this variable are: Luxury, High, Medium, and Low.

2. Variables providing similar average prices can be grouped together. (i.e. 4WD and Front)

3. Subsetting certain variables by other variables show significant price differences (i.e. cylinders by origin)

### Numerical Variables <a name="Numerical-variables"></a>
The same proccess was repeated for the numerical variables.

```{r}
numeric <- cars.train[c(#List of numerical variables)]
numeric.cor <- cor(numeric, method = c("spearman"))
corrplot.mixed(numeric.cor, upper="number", lower="pie")
```

![corrplot.png](/projects/assets/images/corrplot.png)

Next, the variabels were plotted individually in the same way as the categorical variables. Here are some sample graphs:

```{r}
ggplot(cars.train, aes(x=Luggage.room, y=PriceNew)) + geom_point() + geom_smooth()
ggplot(cars.train, aes(x=Passengers, y=PriceNew)) + geom_point() + geom_smooth()
```

![luggageroom.png](/projects/assets/images/luggageroom.png)
![passengers.png](/projects/assets/images/passengers.png)

From the plots it is clear that passengers could be plotted as a factor and treated as a categorical variable.

```{r}
ggplot(cars.train, aes(x=as.factor(Passengers), y=PriceNew)) +
geom_boxplot(aes(fill= as.factor(Passengers),shape = as.factor(Passengers)))
```

![passengers-cat.png](/projects/assets/images/passengers-cat.png)

## Creating New Variables <a name="Creating-new-variables"></a>
With the obervations made from the EDA steps, new variables were created by combining relevant variables, redefining variables, and finally dummy variables were used in place of the remaining categorical variables. This was one of the most important steps as it defined the model parameters moving forward.

## Variable Selection <a name="Variable-selection"></a>
From here the full model was created and tested with various power transformations, as well as outlier tests. 
This step confirmed the base model which the variabel selction proccess would be based off of. 

```{r}
# Plot of leverage vs standardized residuals
cars.train.new$lev <- hatvalues(full_model)
cars.train.new$stdres <- rstandard(full_model)
rvl_plot <- qplot(lev, stdres, data=cars.train.new) + geom_hline(yintercept=c(-2,2), linetype="dashed", col="red") 
# Add boundary lines
rvl_plot <- rvl_plot + geom_vline(xintercept=4/dim(cars.train.new)[1], linetype="dashed", col="blue")
rvl_plot <- rvl_plot + labs(title="Standardized Residuals vs. Leverage")
rvl_plot
```

![lev-plot.png](/projects/assets/images/lev-plot.png)

The variable selection proccess was performed with various regression subset functions and step functions. More can be read [here](https://towardsdatascience.com/feature-selection-techniques-in-regression-model-26878fe0e24e). This was critical in determining the optimal number of variables in the model. 

After performing the various variable selection proccesses, the final model was a combination of 9 variables. Each variable had a low variable inflation factor, and any more reductions in the model were too significant to be made. 

The final model was tested with marginal model plots, leverage plots for each variable, and diagnostic plots (Residuals vs. Fitted, Normal Q-Q, Scale Location, Residuals vs. Leverage). 

```{r}
# Lev plots
leveragePlots(prediction_model)
# Y vs. Y Hat
plot(cars.train.new$PriceNew, predict(prediction_model, newdata = cars.train.new))
 # Diagnostic plots
par(mfrow=c(2,2)) plot(prediction_model)
# Plot of leverage vs standardized residuals
cars.train.new$lev <- hatvalues(prediction_model)
cars.train.new$stdres <- rstandard(prediction_model)
rvl_plot <- qplot(lev, stdres, data=cars.train.new) + geom_hline(yintercept=c(-2,2), linetype="dashed", col="red") 
# Add boundary lines
rvl_plot <- rvl_plot + geom_vline(xintercept=4/dim(cars.train.new)[1], linetype="dashed", col="blue")
rvl_plot <- rvl_plot + labs(title="Standardized Residuals vs. Leverage")
rvl_plot
```

![lev-plot-vars.png](/projects/assets/images/lev-plot-vars.png)
![yhat.png](/projects/assets/images/yhat.png)
![diag-plots.png](/projects/assets/images/diag-plots.png)
![lev-plot-final.png](/projects/assets/images/lev-plot-final.png)

## Testing and Fitting Model <a name="Testing-and-fitting-model"></a>
The final model achieved an R-Squared score of 0.93 with a total of 9 variables which was sufficient enough to land me in the top 10 of all participants in the competition.

## Reflections <a name="Reflections"></a>
This entire model-building procces was quite tedious and rigorous. Compiling the model and searching for the optimal variables to create and select, as well as finding the right transformations to make was equally an art as it was a proccess. There are many combinations of tools and techniques that could be employed to build a similar model so knowing when to use certain tools and ensuring all assumptions were satisfied was critical to my success. I also found that it's quite easy to get lost in miniscule details so circling back to the big picture was very important. In practice it is often enough to get rough answers so being able to balance the technical specs with the business goals is also very important. 