---
layout: post
title: Laplacian Spectral Clustering
category: project
---

In this blog post we will use Python matrix operations to create a [Laplacian spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering) algorithm for clustering data points.


### Notation
In all the math below:

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers).
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers).
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (A@B). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (A@v).

## Introduction

Spectral clustering is a popular exploratory data anlysis technique for dividing data points into groups. In particular, it reduces complex multidimensional datasets into smaller clusters of similar dimensions. We'll start with an example where spectral clustering is not needed.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_4_1.png](/projects/assets/images/output_4_1.png)
    


We can see how the data is roughly split into two circular clusters or "blobs". Clustering is the proccess of separating the data into such distinct groups. A technique known as K-means is a common way to acheive this particular task.


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_6_1.png](/projects/assets/images/output_6_1.png)
    


As shown above, K-means clustering separates the two blobs into two distinct clusters.

## Harder Clustering

Here is an example where the data is irregularly shaped.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![output_9_1.png](/projects/assets/images/output_9_1.png)


Clearly there are two groups in our data, however, they are shaped like crescents. K-means clustering will not work well on this data because by nature it looks for circular clusters of data. We can show how K-means does not sufficiently partition our data.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_11_1.png](/projects/assets/images/output_11_1.png)
    

The K-means algorithm appears to cluster our data partially, with some data points in the wrong partition. In the following section, we will derive and implement spectral clustering to tackle such a clustering problem.

## Part A

We will first construct a *similarity matrix* $$\mathbf{A}$$ of shape (n,n).

A similarity matrix $$\mathbf{P}$$ is one such that for two n-by-n matrices $$\mathbf{A}$$ and $$\mathbf{B}$$, $${\displaystyle B=P^{-1}AP.}$$

In this exercise we will use the epsilon parameter ( = 0.4) to construct $$\mathbf{A}$$. Entry A[i,j] should be equal to 1 if X[i] (the coordinates of data point i) is within distance epsilon of X[j] (the coordinates of data point j), and 0 otherwise.

**The diagonal entries A[i,i] should all be equal to zero.** The function `np.fill_diagonal()` is a good way to set the values of the diagonal of a matrix.

The `pairwise_distances` method from `sklearn` computes pairwise distances and returns a distance matrix. We can use it to find $$\mathbf{A}$$.


```python
# sklearn method to compute pairwise distance
from sklearn.metrics import pairwise_distances

# Set epsilon value
epsilon = 0.4

# Compute pairwise 
A = pairwise_distances(X)

# Fill diagonal entries
np.fill_diagonal(A, 0)

# If distance greater than epsilon set to 0, otherwise set to 1
A[A >= epsilon] = 0
A[(A != 0) & (A < epsilon)] = 1
```

## Part B

The similarity matrix $$\mathbf{A}$$ from above contains information about each point's distance. Our clustering problem on our dataset X will now shift focus on partitioning $$\mathbf{A}$$.

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the degree of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by y. We think of y[i] as being the label of point i. So, if y[i] = 1, then point i (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.

The binary norm cut objective of a matrix $$\mathbf{A}$$ is the function

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression,

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the cut of the clusters $$C_0$$ and $$C_1$$.
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the degree of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The volume of cluster $$C_0$$ is a measure of the size of the cluster.

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately.

### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$.

We will write a function called `cut(A,y)` which will compute the cut term by summing up the entries A[i,j] for each pair of points (i,j) in different clusters. This function will implement for loops to iterate through each row and column entry and check to see which cluster an element belongs in. 



```python
def cut(A, y):
    """
    Function that computes the cut term based on a similarity matrix and cluster memberships specified by y
    """
    cluster_sum = 0
    for i in range(len(y)):
        for j in range(len(y)):
            if y[i] != y[j]: # Ensure that entries are in different clusters
                cluster_sum += A[i][j]
    return(cluster_sum)
```

Now compute the cut values for the true clusters y and compare it with the cut values for a vector of random labels of length n, with each label equal to either 0 or 1. We observe that the true value is approximately 100 times smaller than the vlaue for random labels.


```python
rand = np.random.randint(2, size = n) # Random vector of 1s and 0s
cut(A,y), cut(A, rand)
```




    (26.0, 2188.0)



### B.2 The Volume Term

Now take a look at the second factor in the norm cut objective. This is the volume term. As mentioned above, the volume of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value.

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$.
2. Neither $$C_0$$ and $$C_1$$ are too small.


We will now write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. We will also write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix A with clustering vector y.

Recall that $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the degree of row $i$ (the total number of all other rows related to row $$i$$ through $$A$$). The volume of cluster $$C_0$$ is a measure of the size of the cluster. This is our `vols()` function.


```python
def vols(A, y):
    """
    Function that computes the volumnes of the clusters, retruning them as a tuple
    """
    vol0 = A[y == 0].sum() # Sum of entries in cluster 0
    vol1 = A[y == 1].sum() # Sum of entries in cluster 1
    return(vol0, vol1)
```

Also recall,
$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

This is our `normcut()` function.


```python
def normcut(A, y):
    """
    Function that computes the binary normalized cut objectives of a matrix with clustering vector y
    """
    # Compute the cut value
    cut_ = cut(A,y)
    # Compute the volume terms
    vol0, vol1 = vols(A,y)
    return cut_ * ((1/vol0) + (1/vol1))
```

Now, let's compare the normcut objective using both the true labels y and the fake labels generated above


```python
normcut(A,y), normcut(A,rand)
```




    (0.02303682466323045, 1.9687047722273583)



Observe that the normcut for true labels is nearly 100 times smaller than the normcut for the fake labels that were randomly generated. This makes intuitive sense since a few sections above we showed that the cut for the true lables were also approximately 100 times smaller than the cut for the fake labels.

## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. One approach to clustering is to try to find a cluster vector y such that normcut(A,y) is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick!

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that:

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &amp;\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &amp;\quad \text{if } y_i = 1 \\ 
\end{cases}
$$
Note that the signs of the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$.

Next, if you like linear algebra, you can show that

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$
where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.

1. Write a function called transform(A,y) to compute the appropriate $$\mathbf{z}$$ vector given A and y, using the formula above.
2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.
3. While you're here, also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of n ones (i.e. np.ones(n)). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries.

We should note that the mathematical function above is an exact function. However, Python will result in round-off errors as computer arithmetic produces numerical errors when performing real number calculations. We can use `np.isclose(a,b)` to check if a is approximately close to b.


```python
def transform(A,y):
    """
    Function that computes the z vector given A and y
    """
    # Compute the volume terms
    vol0, vol1 = vols(A,y)
    # Replicate the piecewise function stated above
    z = (1/vol0)*(y == 0) 
    z = z - (1/vol1)*(y == 1)
    return z
```

We can use `np.diag()` to create a diagonal matrix, and `A.sum(axis=1)` to compute the  nonzero entries $$d_{ii} = d_i$$, and where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  


```python
# Diagonal matrix with dii is the row-sum
D = np.diag(A.sum(axis = 1))
# z vector
z = transform(A,y)
# Compare normcut function to the formula
np.isclose(normcut(A, y), z.T@(D - A)@z / (z.T@D@z))
```




    False



Interestingly, multiplying the algebraic formula by 2 renders the two values to be approximately equal.


```python
np.isclose(normcut(A, y), 2*z.T@(D - A)@z / (z.T@D@z))
```




    True



Observe that the output of the `normcut()` is approximately close to the numerical formula.


```python
z.T@D@np.ones(n)
```




    0.0



## Part D¶
In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$
subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. 

Use the minimize function from `scipy.optimize` to minimize the function orth_obj with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name z_.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
z_ = minimize(orth_obj, z)
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have any value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem.

## Part E
Recall that, by design, only the sign of z_min[i] actually contains information about the cluster label of data point i. Plot the original data, using one color for points such that z_min[i] < 0 and another color for points such that z_min[i] >= 0.

Does it look like we came close to correctly clustering the data?


```python
# Assign color based on cluster
color = ["red" if x < 0 else "blue" for x in z_.x]
plt.scatter(X[:,0], X[:,1], c = color)
```

![output_39_1.png](/projects/assets/images/output_39_1.png)
    
With the condition of clustering based on 0, the clustering algorithm does not appear to work well. Let's try with a different value.


```python
color = ["red" if x < -0.0016 else "blue" for x in z_.x]
plt.scatter(X[:,0], X[:,1], c = color)
```

![output_41_1.png](/projects/assets/images/output_41_1.png)  


With the condition set to -0.0016, the clustering algorithm appears to work much better. 

## Part F¶
Explicitly optimizing the orthogonal objective is way too slow to be practical. If spectral clustering required that we do this each time, no one would use it.

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called spectral clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices.

Recall that what we would like to do is minimize the function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$
with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.

The *Rayleigh-Ritz Theorem* states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$
which is equivalent to the standard eigenvalue problem

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$
Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$.

So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with the second-smallest eigenvalue.

We will construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) Laplacian matrix of the similarity matrix $$\mathbf{A}$$. 

Then we will find the eigenvector corresponding its secon-smallest eigenvalue, and call it `z_eig`.


```python
# Construct the Laplacian matrix
L = np.linalg.inv(D)@(D-A)
# Compute the eigen values and eigen vector
Lam, U = np.linalg.eig(L)
# Sort eigen values
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]
# 2nd smallest  eigenvector
z_eig = U[:,1]
```

Now let's plot the data using the sign of z_eig as our color.


```python
color = ["red" if x < 0 else "blue" for x in z_eig]
plt.scatter(X[:,0], X[:,1], c = color)
```

 ![output_46_1.png](/projects/assets/images/output_46_1.png)   


This is much better than before! We can see how the two crescent-shaped clusters are shaded differently with minimal errors. We find that it is much more effective to perform the Laplacian spectral clustering task by finding the eigenvector corresponding to its second-smallest eigenvalue, rather than by explicilty optimizing the orthogonal objective. In the next section we will summarize the entire proccess in one function.

## Part G

We will now synthesize our results from above by writing a function called `spectral_clustering(X, epsilon)`. This function will perform the following steps: 

- Construct the similarity matrix.
- Construct the Laplacian matrix.
- Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.
- Return labels based on this eigenvector.


And return the spectral clustering data we desire.


```python
def spectral_clustering(X, epsilon):
    """
    This function computes the labels for spectral clustering based on:
    X - an array filled with our initial data
    epsilon - float specifying a distance threshold
    ---
    The function will compute the similarity matrix, Laplacian matrix, and minimize the eigenvector 
    with the second-smallest eigenvalue of the Laplacian matrix
    """
    
    # Construct similarity matrix
    A = pairwise_distances(X) # Get pairwise distances
    np.fill_diagonal(A, 0) # Fill diagonal entries with zero
    A[A >= epsilon] = 0 # Set distances greater than epsilon as zero
    A[(A != 0) & (A < epsilon)] = 1 # Set distances within epsilon as 1
    
    # Construct Laplacian matrix
    D = np.diag(A.sum(axis = 1)) # Compute the diagonal matrix with row-sums
    L = np.linalg.inv(D)@(D-A) # Compute the Laplacian matrix
    
    # Compute minimized eigenvector with second-smallest eigenvalue of the Laplacian matrix
    Lam, U = np.linalg.eig(L) # Compute eigenvlaues and corresponding eigenvectors
    ix = Lam.argsort() # Sort eigenvalues
    Lam, U = Lam[ix], U[:,ix] 
    z_eig = U[:,1] #eigenvector with second-smallest eigenvalue of the Laplacian matrix
    
    return(1 * (z_eig>0))
```

Let's test this function on our data to see it in action.


```python
labels = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = labels)
```

![output_51_1.png](/projects/assets/images/output_51_1.png)

Great! This is the result we desried. We successfully summarized the Lapalacian spectral clustering task in one function. In one function we construct the similarity matrix, construct the Laplacian matrix, and compute the minimized eigenvector with second-smallest eigenvalue of the Laplacian matrix.

## Part H

Now we will run a few more experiments using our `spectral_clustering` algorithm on randomly generated datasets. We use the `make_moons` function to create our data. We are interested in seeing how noise affects our function so we will increase our sample size to 1000 and tweak the noise. We start with `noise = 0.01`. 


```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.01, random_state=None)
label = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = label)
```

![moon.png](/projects/assets/images/moon.png)


This generates a result similar to what we saw earlier! Observe that with a small value for noise, our data is extremely compressed. Let's try incresing our noise to `noise = 0.1`.


```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)
label = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = label)
```

![output_53_1.png](/projects/assets/images/output_53_1.png)
    

While the data is more sparse at `noie = 0.1`, we see that our clustering algorithm sill successfully differentiates between the two clusters of data. It is clear that as our noise increases our data becomes more and more sparse. Finally, let's incress our noise once more to `noise = 0.25.`

```python
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.25, random_state=None)
label = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = label)
```

![output_55_1.png](/projects/assets/images/output_55_1.png)

It appears that at this noise level the data is no longer crescent-shaped, rendering it harder for our model to make a distinction in our data. It still splits our data down the middle, as we can see by the line between the colors, but with this data it is much harder to say whether or not our model is performing well.

## Part I
Finally, we will try the spectral clustering function on another data set -- the bull's eye!


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```


![output_58_1.png](/projects/assets/images/output_58_1.png)


We observe that there are two circles of data. We can see how K-means does not properly subset the data.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_60_1.png](/projects/assets/images/output_60_1.png) 


We can experiment with different values of epsilon between 0 and 1.0 to correctly separate the rings. Let's start with 0.1.  


```python
epsilon = 0.1
labels = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = labels)
```

![output_62_1.png](/projects/assets/images/output_62_1.png)

Clearly the model considered the entire data as one label. Let's increase our epsilon to 0.3.


```python
epsilon = 0.3
labels = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = labels)
```

![output_64_1.png](/projects/assets/images/output_64_1.png)

This is perfect! The bullseye data is partitioned so that the outer circle and inner circle have different colors. In this way we see that by adjusting our distance threshold `epsilon`, out Lapalacian spectral clustering algorithm works on different shapes of data. 
