---
layout: post
title: Convolutional Neural Networks in TensorFlow
---

In this blog post we will learn how to build a CNN [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) model in tensorflow for image classification. In particualr, we will be using a [dataset](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) containing images of cats and dogs to perform binary image classification.

# ยง0. Preliminaries

Before we build our model, it is recommended to enable a GPU runtime by navigating to Runtime --> Change Runtime Type in our Google CoLab environment. This will lead to significant speed benefits throughout this exercise. 


# ยง1. Load Packages and Obtain Data

Now, let's start by importing the necessary packages for this tutorial as well as run pre-written code to download and prepare the data.


```python
import os
from tensorflow.keras import utils 
import tensorflow as tf
from matplotlib import pyplot as plt
import numpy as np
from tensorflow.keras import layers, models
```


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


By running this code, we have created TensorFlow Datasets containing image files for training, validation, and testing. The dataset can be treated as a pipeline that feeds images to our machine learning model. 

Next, run the following code block to rapidly read in the data.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

## Working with Datasets

We will begin exploring our data by plotting the images in our data. In particular, let's write a function to create a two-row visualization where the first row shows images of dogs and the second row shows images of cats. 

In order to iterate through our dataset and access the images,  use the `.take(1)` method which will return 32 images along with their corresponding labels. Furthermore, in order to access the dog and cat images separately, we will extract the labels and save their indeces. Using this we can plot dogs and cats separately.



```python
def plot_dogs_cats():
  """
  Plot function for two-row visualization of dogs and cats
  """
  # Label classes
  class_names = ["cat", "dog"] 

  # Plot random images of dogs in first row, cats in second row
  for images, labels in train_dataset.take(1): 

    fig, ax = plt.subplots(2,3,figsize=(10, 10))

    # Get the index of dogs and cats
    dogs = np.where(labels.numpy() == 1) 
    cats = np.where(labels.numpy() == 0) 

  # Plot dogs
  for i in range(3):
    ax[0,i].imshow(images[dogs[0][:3][i]].numpy().astype("uint8"))
    ax[0,i].set_title(class_names[labels[dogs[0][:3][i]]])
    ax[0,i].axis("off")

  # Plot cats
  for i in range(3):
    ax[1,i].imshow(images[cats[0][:3][i]].numpy().astype("uint8"))
    ax[1,i].set_title(class_names[labels[cats[0][:3][i]]])
    ax[1,i].axis("off")
    
  return

```


```python
plot_dogs_cats()
```


    
![output_9_0.png](/images/output_9_0.png)

    


Our function plots dogs on the first row and cats on the second row so it works successfully! If we run the function again we will get different images.

## Check Label Frequencies

We are also interested in understanding how many images of each class our dataset contains. We can find out exactly how many dog and cat images are in the dataset using the following iterator: `train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()`. In our dataset, dogs are labeled as 1 and cats are labeled as 0. Hence, by enumerating through the iterator we can find how many dogs are in the data, and extract the complement to find how many cats our in the data. 


```python
sum = 0

# Enumerate iterator to find number of dogs and cats
for step, element in enumerate(train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()):
  sum += element

print(f"There are {sum} dogs and {(step + 1) - sum} cats!")
```

    There are 1000 dogs and 1000 cats!


This means that a baseline machine learning model would have an accuracy of `50%` as there are an equal number of dogs and cats. We will treat this as our benchmark accuracy to consider improvements in our machine learning models.

# ยง2. First Model

In our first model we will create a `tf.keras.Sequential` model, which is the building block for tensorflow CNN models. Using this we can add layers to the model by hand. In particualr, our model will consider `Conv2D` layers which are convolutional kernals, `MaxPooling2D` which downsamples the input along its spatial dimensions, `dense` layers which are regular deeply connected neural network layers, `flatten` which flattens our data, and finally `Dropout` which prevents overfitting.

The model can be constructed in the following way:
- Input layer
- Hidden layer
- Output layer


```python
model1 = models.Sequential([
      # Input    
      layers.InputLayer(input_shape=(160,160,3)),  

      # Hidden Layers             
      layers.Conv2D(16, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(16, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(64, (3,3), activation="relu"), 
      layers.MaxPooling2D((2, 2)),

      # Output
      layers.Flatten(),
      layers.Dense(64, activation='relu'),
      layers.Dropout(0.2),
      layers.Dense(2)
])
```

The specific units that are listed in the arguments of each layer were found by trial and error. 

Here is a summary of our model. We can see how many parameters are considered at each layer of the model.


```python
model1.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d (Conv2D)             (None, 158, 158, 16)      448       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 79, 79, 16)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 77, 77, 16)        2320      
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 38, 38, 16)       0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 36, 36, 32)        4640      
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 18, 18, 32)       0         
     2D)                                                             
                                                                     
     conv2d_3 (Conv2D)           (None, 16, 16, 32)        9248      
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 8, 8, 32)         0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 6, 6, 64)          18496     
                                                                     
     max_pooling2d_4 (MaxPooling  (None, 3, 3, 64)         0         
     2D)                                                             
                                                                     
     flatten (Flatten)           (None, 576)               0         
                                                                     
     dense (Dense)               (None, 64)                36928     
                                                                     
     dropout (Dropout)           (None, 64)                0         
                                                                     
     dense_1 (Dense)             (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 72,210
    Trainable params: 72,210
    Non-trainable params: 0
    _________________________________________________________________


We are now ready to train our data. We specify `epochs=20` to determine how many training iterations will be considered.


```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history1 = model1.fit(train_dataset,
                    epochs=20,
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 36s 86ms/step - loss: 1.3004 - accuracy: 0.5415 - val_loss: 0.6832 - val_accuracy: 0.5446
    Epoch 2/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6850 - accuracy: 0.5545 - val_loss: 0.6951 - val_accuracy: 0.5285
    Epoch 3/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6708 - accuracy: 0.5890 - val_loss: 0.6614 - val_accuracy: 0.6287
    Epoch 4/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6484 - accuracy: 0.6280 - val_loss: 0.7023 - val_accuracy: 0.5495
    Epoch 5/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6327 - accuracy: 0.6415 - val_loss: 0.6579 - val_accuracy: 0.6213
    Epoch 6/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6108 - accuracy: 0.6525 - val_loss: 0.6822 - val_accuracy: 0.5854
    Epoch 7/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5788 - accuracy: 0.6855 - val_loss: 0.6319 - val_accuracy: 0.6510
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5608 - accuracy: 0.7105 - val_loss: 0.6228 - val_accuracy: 0.6547
    Epoch 9/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5205 - accuracy: 0.7395 - val_loss: 0.6600 - val_accuracy: 0.6411
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5051 - accuracy: 0.7490 - val_loss: 0.6032 - val_accuracy: 0.6894
    Epoch 11/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.5627 - accuracy: 0.7030 - val_loss: 0.6235 - val_accuracy: 0.6856
    Epoch 12/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.4782 - accuracy: 0.7670 - val_loss: 0.6042 - val_accuracy: 0.6918
    Epoch 13/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.4346 - accuracy: 0.7960 - val_loss: 0.6555 - val_accuracy: 0.6980
    Epoch 14/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.4077 - accuracy: 0.8035 - val_loss: 0.7416 - val_accuracy: 0.6572
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3898 - accuracy: 0.8195 - val_loss: 0.6989 - val_accuracy: 0.6931
    Epoch 16/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3152 - accuracy: 0.8550 - val_loss: 0.7461 - val_accuracy: 0.6757
    Epoch 17/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2811 - accuracy: 0.8730 - val_loss: 0.8222 - val_accuracy: 0.6881
    Epoch 18/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2672 - accuracy: 0.8835 - val_loss: 0.7950 - val_accuracy: 0.6795
    Epoch 19/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2207 - accuracy: 0.9080 - val_loss: 1.0388 - val_accuracy: 0.6696
    Epoch 20/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2300 - accuracy: 0.8975 - val_loss: 0.8772 - val_accuracy: 0.6795


Let's plot our training and validation accuracy to see how this model performs over each iteration.




```python
acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')
```




    Text(0.5, 1.0, 'Training and Validation Accuracy')




    
![output_21_1.png](/images/output_21_1.png)
    


**The accuracy of this model ranges from approximately 60%-70% during training.**

Since the baseline accuracy is 50%, our model has clearly performed better. We can also see from the graph above that our model is overfitting, as the training accuracy is much higher for more iterations of our model, and it also appears to increase much faster. In the next section we will introduce additional methods to alter our data to train our model.

# ยง3. Model with Data Augmentation

Now we will add a data augmentation layer. This layer will modify the images in a way where the model will have to learn invariant features of our input images. For example, if an image of a cat is flipped, our model will have to learn what an upside-down cat looks like, along with its flipped features. We will first introduce `tf.keras.layers.RandomFlip()`. This randomly flips images based on a specific condition. By specifying `horizontal_and_vertical` images will be flipped both horizontally and vertically.


```python
data_flip = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal_and_vertical')
])
```


```python
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10,10))
  first_image = images[0]
  for i in range(4):
    ax = plt.subplot(1,4,i+1)
    augmented_image = data_flip(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![output_25_0.png](/images/output_25_0.png)
    


As we can see this cat is flipped along the horizontal and vertical axis.

Now we will introduce `tf.keras.layers.RandomRotation()` which randomly rotates our images. 


```python
data_rotation = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.2)
])
```


```python
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10,10))
  first_image = images[0]
  for i in range(4):
    ax = plt.subplot(1,4,i+1)
    augmented_image = data_rotation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```



![output_28_0.png)](/images/output_28_0.png)
    


As we can see the cat is rotated at slight 20 degree angles.

We will create one layer that applies both transformations in one step and apply this to a new model called `model2`.


```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal_and_vertical'),                                      
  tf.keras.layers.RandomRotation(0.2)
])
```

We can train the model in the way we did above, but with the data augmentation layer first.


```python
model2 = models.Sequential([
      # Input    
      layers.InputLayer(input_shape=(160,160,3)),  

      # Data augmentation
      data_augmentation,

      # Hidden Layers             
      layers.Conv2D(16, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(16, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(32, (3,3), activation="relu"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(64, (3,3), activation="relu"), 
      layers.MaxPooling2D((2, 2)),

      # Output
      layers.Flatten(),
      layers.Dense(64, activation='relu'),
      layers.Dropout(0.2),
      layers.Dense(2)
])
```


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history2 = model2.fit(train_dataset,
                    epochs=20,
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 82ms/step - loss: 1.0940 - accuracy: 0.5245 - val_loss: 0.6853 - val_accuracy: 0.5322
    Epoch 2/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6845 - accuracy: 0.5650 - val_loss: 0.6771 - val_accuracy: 0.5705
    Epoch 3/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6820 - accuracy: 0.5695 - val_loss: 0.6719 - val_accuracy: 0.5854
    Epoch 4/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6715 - accuracy: 0.5885 - val_loss: 0.6604 - val_accuracy: 0.5916
    Epoch 5/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6683 - accuracy: 0.5850 - val_loss: 0.6796 - val_accuracy: 0.5668
    Epoch 6/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6729 - accuracy: 0.5890 - val_loss: 0.6910 - val_accuracy: 0.5594
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6841 - accuracy: 0.5750 - val_loss: 0.6771 - val_accuracy: 0.5644
    Epoch 8/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6768 - accuracy: 0.5650 - val_loss: 0.6645 - val_accuracy: 0.6101
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6688 - accuracy: 0.6020 - val_loss: 0.6600 - val_accuracy: 0.5891
    Epoch 10/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6598 - accuracy: 0.6100 - val_loss: 0.6665 - val_accuracy: 0.5829
    Epoch 11/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6613 - accuracy: 0.6070 - val_loss: 0.6757 - val_accuracy: 0.5804
    Epoch 12/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6643 - accuracy: 0.6030 - val_loss: 0.6659 - val_accuracy: 0.5903
    Epoch 13/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6633 - accuracy: 0.5945 - val_loss: 0.6473 - val_accuracy: 0.6262
    Epoch 14/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6580 - accuracy: 0.6285 - val_loss: 0.6549 - val_accuracy: 0.6349
    Epoch 15/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6496 - accuracy: 0.6410 - val_loss: 0.6464 - val_accuracy: 0.6225
    Epoch 16/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6532 - accuracy: 0.6175 - val_loss: 0.6656 - val_accuracy: 0.5978
    Epoch 17/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6506 - accuracy: 0.6150 - val_loss: 0.6552 - val_accuracy: 0.6114
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6525 - accuracy: 0.6445 - val_loss: 0.6592 - val_accuracy: 0.6324
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6415 - accuracy: 0.6475 - val_loss: 0.6416 - val_accuracy: 0.6287
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6453 - accuracy: 0.6300 - val_loss: 0.6797 - val_accuracy: 0.5606



```python
acc2 = history2.history['accuracy']
val_acc2 = history2.history['val_accuracy']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc2, label='Training Accuracy')
plt.plot(val_acc2, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')
```




    Text(0.5, 1.0, 'Training and Validation Accuracy')





![output_34_1.png](/images/output_34_1.png)

    


**The validation accuracy of model2 ranges from approximately 52%-63% during training**

Since the baseline accuracy is 50%, model2 is similarly an improvement. However, it evidently performs worse than model1 which did not include the flip or rotation layers, as can be seen in the accuracy plot - we see that the training validation does not increase in the same way as it did in model1. In terms of fit, model2 does not appear to overfit as the validation accuracy does not deviate from the training accuracy too much. Let's add another step to our modeling pipeline.

# ยง4. Data Preprocessing

In this section we will preproccess our input data by transforming it. For example, images often have pixels with RGB values between 0 and 255, but we can scale it to be between 0 and 1 or -1 and 1. Performing this scaling will allow our model to focues more on handling the data and less time scaling and adjusting it. 

The following code creates such a preprocessing layer.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Let's add the preprocessing layer before the data augmentation step, and create a new model, `model3`.


```python
model3 = models.Sequential([
      # Input    
      layers.InputLayer(input_shape=(160,160,3)),  

      # Preprocess Data
      preprocessor,

      # Data augmentation
      data_augmentation,

      # Hidden Layers             
      layers.Conv2D(16, (3,3), activation="relu", padding="same"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(16, (3,3), activation="relu", padding="same"),
      layers.MaxPooling2D((2, 2)),


      # Output
      layers.Flatten(),
      layers.Dense(64, activation='relu'),
      layers.Dropout(0.2),
      layers.Dense(2)
])
```


```python
model3.summary()
```

    Model: "sequential_7"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     sequential_3 (Sequential)   (None, 160, 160, 3)       0         
                                                                     
     conv2d_14 (Conv2D)          (None, 160, 160, 16)      448       
                                                                     
     max_pooling2d_14 (MaxPoolin  (None, 80, 80, 16)       0         
     g2D)                                                            
                                                                     
     conv2d_15 (Conv2D)          (None, 80, 80, 16)        2320      
                                                                     
     max_pooling2d_15 (MaxPoolin  (None, 40, 40, 16)       0         
     g2D)                                                            
                                                                     
     flatten_4 (Flatten)         (None, 25600)             0         
                                                                     
     dense_8 (Dense)             (None, 64)                1638464   
                                                                     
     dropout_4 (Dropout)         (None, 64)                0         
                                                                     
     dense_9 (Dense)             (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 1,641,362
    Trainable params: 1,641,362
    Non-trainable params: 0
    _________________________________________________________________



```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history3 = model3.fit(train_dataset,
                    epochs=20,
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 83ms/step - loss: 0.7655 - accuracy: 0.5245 - val_loss: 0.6664 - val_accuracy: 0.5260
    Epoch 2/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6602 - accuracy: 0.5885 - val_loss: 0.6403 - val_accuracy: 0.5817
    Epoch 3/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6344 - accuracy: 0.6250 - val_loss: 0.6301 - val_accuracy: 0.6262
    Epoch 4/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6221 - accuracy: 0.6530 - val_loss: 0.6055 - val_accuracy: 0.6535
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6132 - accuracy: 0.6590 - val_loss: 0.6079 - val_accuracy: 0.6745
    Epoch 6/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6056 - accuracy: 0.6625 - val_loss: 0.6002 - val_accuracy: 0.6460
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5926 - accuracy: 0.6845 - val_loss: 0.5945 - val_accuracy: 0.7030
    Epoch 8/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5906 - accuracy: 0.6790 - val_loss: 0.5991 - val_accuracy: 0.6733
    Epoch 9/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5856 - accuracy: 0.6775 - val_loss: 0.6084 - val_accuracy: 0.6473
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5797 - accuracy: 0.6875 - val_loss: 0.5842 - val_accuracy: 0.6918
    Epoch 11/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5794 - accuracy: 0.6890 - val_loss: 0.5943 - val_accuracy: 0.6881
    Epoch 12/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5717 - accuracy: 0.6990 - val_loss: 0.6107 - val_accuracy: 0.6708
    Epoch 13/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5694 - accuracy: 0.6935 - val_loss: 0.5720 - val_accuracy: 0.6980
    Epoch 14/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5597 - accuracy: 0.7040 - val_loss: 0.5796 - val_accuracy: 0.6993
    Epoch 15/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5751 - accuracy: 0.6865 - val_loss: 0.5838 - val_accuracy: 0.6856
    Epoch 16/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5519 - accuracy: 0.7205 - val_loss: 0.5785 - val_accuracy: 0.6993
    Epoch 17/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5601 - accuracy: 0.7045 - val_loss: 0.5507 - val_accuracy: 0.7042
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5400 - accuracy: 0.7255 - val_loss: 0.5582 - val_accuracy: 0.7129
    Epoch 19/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5498 - accuracy: 0.7080 - val_loss: 0.5609 - val_accuracy: 0.7265
    Epoch 20/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5398 - accuracy: 0.7165 - val_loss: 0.5696 - val_accuracy: 0.7129



```python
acc3 = history3.history['accuracy']
val_acc3 = history3.history['val_accuracy']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc3, label='Training Accuracy')
plt.plot(val_acc3, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')
```




    Text(0.5, 1.0, 'Training and Validation Accuracy')




  
![output_42_1.png](/images/output_42_1.png)
    


**The accuracy of this model ranged from approximately 65%-75% during training.**

Since the baseline accuracy is 50%, model3 is again an improvement. The validation accuracy achieved by this model is higher than the validation accuracy of model1 as it reaches at least 70% after five training iterations Furthermore, we observed overfitting in model1 but in model3 we do not see overfitting, as the accuracy plot shows the lines are very close to eachother for all iterations of training. This is our strongest model so far. However, we can still do better. In the next section we will discuss **transfer learning** which allows us to build on previously trained models.

# ยง5. Transfer Learning

For this section we will download `MobileNetV2` which will be our pre-trained base model. The following code dowonloads it to our environment.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step


Now, all we have to do is add the base model as a layer in our tensorflow model.


```python
model4 = models.Sequential([
      # Input    
      layers.InputLayer(input_shape=(160,160,3)),  

      # Preprocess Data
      preprocessor,

      # Data augmentation
      data_augmentation,

      # Base model
      base_model_layer,

      # Hidden Layers             
      layers.Conv2D(16, (3,3), activation="relu", padding="same"),
      layers.MaxPooling2D((2, 2)),
      layers.Conv2D(16, (3,3), activation="relu", padding="same"),
      layers.MaxPooling2D((2, 2)),

      # Output
      layers.Flatten(),
      layers.Dense(64, activation='relu'),
      layers.Dropout(0.2),
      layers.Dense(2)
])
```


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history4 = model4.fit(train_dataset,
                    epochs=20,
                    validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 124ms/step - loss: 0.3659 - accuracy: 0.8355 - val_loss: 0.1007 - val_accuracy: 0.9592
    Epoch 2/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.2335 - accuracy: 0.8995 - val_loss: 0.0819 - val_accuracy: 0.9629
    Epoch 3/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1716 - accuracy: 0.9375 - val_loss: 0.0777 - val_accuracy: 0.9715
    Epoch 4/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1972 - accuracy: 0.9260 - val_loss: 0.0590 - val_accuracy: 0.9790
    Epoch 5/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1606 - accuracy: 0.9335 - val_loss: 0.0684 - val_accuracy: 0.9740
    Epoch 6/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1657 - accuracy: 0.9380 - val_loss: 0.0645 - val_accuracy: 0.9752
    Epoch 7/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1642 - accuracy: 0.9350 - val_loss: 0.0642 - val_accuracy: 0.9728
    Epoch 8/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1445 - accuracy: 0.9445 - val_loss: 0.0572 - val_accuracy: 0.9765
    Epoch 9/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1439 - accuracy: 0.9455 - val_loss: 0.0574 - val_accuracy: 0.9752
    Epoch 10/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1282 - accuracy: 0.9440 - val_loss: 0.0618 - val_accuracy: 0.9740
    Epoch 11/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.1195 - accuracy: 0.9550 - val_loss: 0.0670 - val_accuracy: 0.9777
    Epoch 12/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1209 - accuracy: 0.9560 - val_loss: 0.0627 - val_accuracy: 0.9752
    Epoch 13/20
    63/63 [==============================] - 7s 98ms/step - loss: 0.1067 - accuracy: 0.9580 - val_loss: 0.0792 - val_accuracy: 0.9728
    Epoch 14/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1253 - accuracy: 0.9530 - val_loss: 0.0694 - val_accuracy: 0.9703
    Epoch 15/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1051 - accuracy: 0.9565 - val_loss: 0.0814 - val_accuracy: 0.9715
    Epoch 16/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1105 - accuracy: 0.9595 - val_loss: 0.0820 - val_accuracy: 0.9703
    Epoch 17/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1004 - accuracy: 0.9600 - val_loss: 0.0808 - val_accuracy: 0.9740
    Epoch 18/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1070 - accuracy: 0.9590 - val_loss: 0.0748 - val_accuracy: 0.9752
    Epoch 19/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.0997 - accuracy: 0.9605 - val_loss: 0.0909 - val_accuracy: 0.9691
    Epoch 20/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1007 - accuracy: 0.9620 - val_loss: 0.0754 - val_accuracy: 0.9703



```python
acc4 = history4.history['accuracy']
val_acc4 = history4.history['val_accuracy']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc4, label='Training Accuracy')
plt.plot(val_acc4, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')
```




    Text(0.5, 1.0, 'Training and Validation Accuracy')




  
![output_49_1.png](/images/output_49_1.png)
    


**The validation accuracy of model4 ranges from approximately 95%-98% during training.**

Since the baseline accuracy is 50%, model4 is an improvement. The validation accuracy achieved by this model, which builds on a pre-trained model, is our highest one yet at over 95%. There does not appear to be any overfitting, and in fact our validation accuracy is greater than our training accuracy throughout all iterations! At a glance this may seem odd, but this is due to the drop out layer which puts additional restrictions on the training data. This is an extremely accurate model so let's test it on our test data to see how it performs on unsees data.

# ยง6. Score on Test Data
Our last step is to test our trained model against our test data. 


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 69ms/step - loss: 0.1018 - accuracy: 0.9583
    Test accuracy : 0.9583333134651184


The accuracy on the unseen testing data is roughly 96%! This is in line with the validation accuracy we achieved so we are satisfied.
